{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a434380",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a122a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, f1_score\n",
    ")\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re \n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a2513",
   "metadata": {},
   "source": [
    "## caricamento dei dati "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50da173c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset caricato: 500 righe √ó 52 colonne\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('ai_human_with_signatures.csv')\n",
    "print(f\"Dataset caricato: {df.shape[0]} righe √ó {df.shape[1]} colonne\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3ce92",
   "metadata": {},
   "source": [
    "## feature selection strategica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49135cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature stilometriche pure (NO leakage semantico)\n",
    "stylometric_features = [\n",
    "    # Signature avanzate (R-S-L-D-E-C)\n",
    "    'sentence_length_cv',\n",
    "    'burstiness_index',\n",
    "    'pos_bigram_entropy',\n",
    "    'dependency_depth_mean',\n",
    "    'lexical_compression_ratio',\n",
    "    'function_word_ratio',\n",
    "    'sentence_similarity_drift',\n",
    "    'structural_redundancy',\n",
    "    'sentiment_variance',\n",
    "    'readability_oscillation',\n",
    "    'clause_density',\n",
    "    \n",
    "    # Feature linguistiche base\n",
    "    'avg_word_length',\n",
    "    'lexical_diversity',\n",
    "    'punctuation_density',\n",
    "    'exclamation_count',\n",
    "    'question_count',\n",
    "    'sentence_count',\n",
    "    'avg_sentence_length',\n",
    "    'type_token_ratio',\n",
    "    \n",
    "    # Firme stilistiche\n",
    "    'hedge_words_count',\n",
    "    'intensifier_count',\n",
    "    'first_person_count',\n",
    "    'contraction_density',\n",
    "    \n",
    "    # Readability\n",
    "    'readability_score',\n",
    "    'grade_level',\n",
    "    \n",
    "    # Sintattiche\n",
    "    'passive_voice_ratio',\n",
    "    'unique_bigram_ratio',\n",
    "    'sentence_length_variability',\n",
    "    \n",
    "    # Named Entities\n",
    "    'entity_density',\n",
    "    'person_entity_count',\n",
    "    'org_entity_count',\n",
    "    'location_entity_count'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a4f73b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature disponibili: 32/32\n"
     ]
    }
   ],
   "source": [
    "# Verifica disponibilit√† feature\n",
    "available_features = [f for f in stylometric_features if f in df.columns]\n",
    "missing_features = [f for f in stylometric_features if f not in df.columns]\n",
    "\n",
    "print(f\"Feature disponibili: {len(available_features)}/{len(stylometric_features)}\")\n",
    "if missing_features:\n",
    "    print(f\"‚ö†Ô∏è  Feature mancanti: {', '.join(missing_features[:5])}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a173c6b",
   "metadata": {},
   "source": [
    "### variabile dipendente e indipendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "436db9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribuzione target:\n",
      "label_encoded\n",
      "1    0.502\n",
      "0    0.498\n",
      "Name: proportion, dtype: float64\n",
      "   Human (0): 249 campioni\n",
      "   AI (1): 251 campioni\n"
     ]
    }
   ],
   "source": [
    "X = df[available_features].fillna(0)  \n",
    "y = df['label_encoded']\n",
    "\n",
    "print(f\"\\nDistribuzione target:\")\n",
    "print(y.value_counts(normalize=True).round(3))\n",
    "print(f\"   Human (0): {(y==0).sum()} campioni\")\n",
    "print(f\"   AI (1): {(y==1).sum()} campioni\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ac7d4",
   "metadata": {},
   "source": [
    "## split into the train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5b02915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train set: 400 campioni\n",
      "Test set:  100 campioni\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\" Train set: {X_train.shape[0]} campioni\")\n",
    "print(f\"Test set:  {X_test.shape[0]} campioni\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bacae39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hapax_count / \u001b[38;5;28mlen\u001b[39m(words)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Applicazione\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mhapax_density\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext_cleaned\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_hapax_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mextract_hapax_features\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m words:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m counts = \u001b[43mCounter\u001b[49m(words)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Gli Hapax Legomena sono parole con frequenza esattamente 1\u001b[39;00m\n\u001b[32m     12\u001b[39m hapax_count = \u001b[38;5;28mlen\u001b[39m([w \u001b[38;5;28;01mfor\u001b[39;00m w, freq \u001b[38;5;129;01min\u001b[39;00m counts.items() \u001b[38;5;28;01mif\u001b[39;00m freq == \u001b[32m1\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_hapax_features(text):\n",
    "    \"\"\"\n",
    "    Calcola la densit√† di parole che appaiono una sola volta.\n",
    "    Un valore alto indica solitamente un autore umano con un vocabolario specifico.\n",
    "    \"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    if not words:\n",
    "        return 0.0\n",
    "    \n",
    "    counts = Counter(words)\n",
    "    # Gli Hapax Legomena sono parole con frequenza esattamente 1\n",
    "    hapax_count = len([w for w, freq in counts.items() if freq == 1])\n",
    "    \n",
    "    # Restituiamo il rapporto rispetto al totale delle parole (densit√†)\n",
    "    return hapax_count / len(words)\n",
    "\n",
    "# Applicazione\n",
    "df['hapax_density'] = df['text_cleaned'].apply(extract_hapax_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ff405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_template_bias(text):\n",
    "    \"\"\"\n",
    "    Rileva pattern strutturali tipici degli LLM (struttura a 3 punti, \n",
    "    conclusioni standard, introduzioni formali).\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    # 1. Check presenza liste puntate (molto comune nell'AI)\n",
    "    bullet_points = len(re.findall(r'(?m)^(\\s*[-*‚Ä¢]|\\d+\\.)', text))\n",
    "    if bullet_points > 2: score += 1\n",
    "    \n",
    "    # 2. Check di chiusure \"clich√©\" (Conclusioni AI)\n",
    "    ai_conclusions = [\n",
    "        'in conclusion', 'overall', 'to summarize', \n",
    "        'it is important to remember', 'ultimately'\n",
    "    ]\n",
    "    if any(phrase in text.lower() for phrase in ai_conclusions):\n",
    "        score += 1.5\n",
    "        \n",
    "    # 3. Check di strutture di transizione troppo regolari\n",
    "    transitions = ['furthermore', 'moreover', 'additionally', 'consequently']\n",
    "    trans_count = sum(1 for t in transitions if t in text.lower())\n",
    "    if trans_count > 2: score += 1\n",
    "\n",
    "    return score\n",
    "\n",
    "# Applicazione\n",
    "df_final['template_bias_score'] = df_final['text_cleaned'].apply(extract_template_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0e1bcb",
   "metadata": {},
   "source": [
    "## standardizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb749b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizzazione completata (mean=0, std=1)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Standardizzazione completata (mean=0, std=1)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b650c8ac",
   "metadata": {},
   "source": [
    "## TRAINING MULTI-MODELLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d6203b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/8] Training modelli...\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[5/8] Training modelli...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced'\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=150,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        penalty='l2',\n",
    "        C=1.0\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b525067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Training Random Forest...\n",
      "   ROC-AUC (Test): 1.0000\n",
      "   F1-Score:       0.9899\n",
      "   CV ROC-AUC:     0.9982 ¬± 0.0035\n",
      "\n",
      "üîß Training Gradient Boosting...\n",
      "   ROC-AUC (Test): 0.9990\n",
      "   F1-Score:       0.9697\n",
      "   CV ROC-AUC:     0.9836 ¬± 0.0145\n",
      "\n",
      "üîß Training Logistic Regression...\n",
      "   ROC-AUC (Test): 1.0000\n",
      "   F1-Score:       1.0000\n",
      "   CV ROC-AUC:     0.9998 ¬± 0.0005\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîß Training {name}...\")\n",
    "    \n",
    "    # Training\n",
    "    if 'Logistic' in name:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Metriche\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    if 'Logistic' in name:\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, \n",
    "                                     cv=cv, scoring='roc_auc')\n",
    "    else:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, \n",
    "                                     cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'roc_auc': roc_auc,\n",
    "        'f1': f1,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"   ROC-AUC (Test): {roc_auc:.4f}\")\n",
    "    print(f\"   F1-Score:       {f1:.4f}\")\n",
    "    print(f\"   CV ROC-AUC:     {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ece720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " MIGLIOR MODELLO: Random Forest\n",
      "   ROC-AUC: 1.0000\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "best_model_name = max(results, key=lambda k: results[k]['roc_auc'])\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\" MIGLIOR MODELLO: {best_model_name}\")\n",
    "print(f\"   ROC-AUC: {results[best_model_name]['roc_auc']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e2ad3",
   "metadata": {},
   "source": [
    "## 6. ANALISI DETTAGLIATA MIGLIOR MODELLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd11f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Analisi dettagliata: Random Forest\n",
      "----------------------------------------------------------------------\n",
      "\n",
      " CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Human     0.9804    1.0000    0.9901        50\n",
      "          AI     1.0000    0.9800    0.9899        50\n",
      "\n",
      "    accuracy                         0.9900       100\n",
      "   macro avg     0.9902    0.9900    0.9900       100\n",
      "weighted avg     0.9902    0.9900    0.9900       100\n",
      "\n",
      "\n",
      " CONFUSION MATRIX:\n",
      "                Predicted\n",
      "              Human    AI\n",
      "Actual Human     50     0\n",
      "       AI         1    49\n",
      "\n",
      " METRICHE AGGIUNTIVE:\n",
      "   Sensitivity (Recall):  0.9800\n",
      "   Specificity:           1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Analisi dettagliata: {best_model_name}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "y_pred_best = results[best_model_name]['y_pred']\n",
    "y_pred_proba_best = results[best_model_name]['y_pred_proba']\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n CLASSIFICATION REPORT:\")\n",
    "print(classification_report(y_test, y_pred_best, \n",
    "                          target_names=['Human', 'AI'],\n",
    "                          digits=4))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "print(\"\\n CONFUSION MATRIX:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"              Human    AI\")\n",
    "print(f\"Actual Human   {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
    "print(f\"       AI      {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
    "\n",
    "# Metriche derivate\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(f\"\\n METRICHE AGGIUNTIVE:\")\n",
    "print(f\"   Sensitivity (Recall):  {sensitivity:.4f}\")\n",
    "print(f\"   Specificity:           {specificity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bff7f3",
   "metadata": {},
   "source": [
    "## 7. FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e57d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7/8] Analisi Feature Importance...\n",
      "\n",
      "üéØ TOP 10 FEATURE PI√ô IMPORTANTI:\n",
      "    1. first_person_count             0.3127\n",
      "    2. punctuation_density            0.2207\n",
      "    3. dependency_depth_mean          0.0751\n",
      "    4. pos_bigram_entropy             0.0484\n",
      "    5. avg_sentence_length            0.0437\n",
      "    6. hedge_words_count              0.0429\n",
      "    7. avg_word_length                0.0403\n",
      "    8. function_word_ratio            0.0354\n",
      "    9. grade_level                    0.0193\n",
      "   10. clause_density                 0.0174\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n[7/8] Analisi Feature Importance...\")\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Tree-based models\n",
    "    importances = best_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    print(\"\\n TOP 10 FEATURE PI√ô IMPORTANTI:\")\n",
    "    for i, idx in enumerate(indices[:10], 1):\n",
    "        print(f\"   {i:2d}. {available_features[idx]:30s} {importances[idx]:.4f}\")\n",
    "    \n",
    "    feature_importance_data = pd.DataFrame({\n",
    "        'feature': available_features,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # Linear models\n",
    "    coefficients = np.abs(best_model.coef_[0])\n",
    "    indices = np.argsort(coefficients)[::-1]\n",
    "    \n",
    "    print(\"\\n TOP 10 FEATURE PI√ô INFLUENTI:\")\n",
    "    for i, idx in enumerate(indices[:10], 1):\n",
    "        coef_sign = '+' if best_model.coef_[0][idx] > 0 else '-'\n",
    "        print(f\"   {i:2d}. {available_features[idx]:30s} {coef_sign}{coefficients[idx]:.4f}\")\n",
    "    \n",
    "    feature_importance_data = pd.DataFrame({\n",
    "        'feature': available_features,\n",
    "        'importance': coefficients\n",
    "    }).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa29ca",
   "metadata": {},
   "source": [
    "### 8. VISUALIZZAZIONI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf13e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "# --- 1. PREPARAZIONE AMBIENTE E CARTELLE ---\n",
    "# Creiamo le cartelle se non esistono per evitare errori di FileNotFoundError\n",
    "for folder in ['pkl', 'txt', 'immagini']:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        print(f\"üìÅ Cartella creata: {folder}\")\n",
    "\n",
    "# --- 2. GENERAZIONE VISUALIZZAZIONI ---\n",
    "print(\"\\n[8/8] Generazione visualizzazioni...\")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Confusion Matrix Heatmap\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Human', 'AI'],\n",
    "            yticklabels=['Human', 'AI'],\n",
    "            ax=ax1, cbar_kws={'label': 'Count'})\n",
    "ax1.set_title(f'Confusion Matrix - {best_model_name}', fontweight='bold', fontsize=12)\n",
    "ax1.set_ylabel('True Label', fontweight='bold')\n",
    "ax1.set_xlabel('Predicted Label', fontweight='bold')\n",
    "\n",
    "# Plot 2: ROC Curves Comparison\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "for name, res in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, res['y_pred_proba'])\n",
    "    ax2.plot(fpr, tpr, label=f\"{name} (AUC={res['roc_auc']:.3f})\", linewidth=2)\n",
    "ax2.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "ax2.set_xlabel('False Positive Rate', fontweight='bold')\n",
    "ax2.set_ylabel('True Positive Rate', fontweight='bold')\n",
    "ax2.set_title('ROC Curves Comparison', fontweight='bold', fontsize=12)\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Precision-Recall Curve\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_best)\n",
    "ax3.plot(recall, precision, linewidth=2, color='purple')\n",
    "ax3.fill_between(recall, precision, alpha=0.2, color='purple')\n",
    "ax3.set_xlabel('Recall', fontweight='bold')\n",
    "ax3.set_ylabel('Precision', fontweight='bold')\n",
    "ax3.set_title(f'Precision-Recall Curve - {best_model_name}', fontweight='bold', fontsize=12)\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: Feature Importance (Top 15)\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "top_features = feature_importance_data.head(15)\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_features)))\n",
    "bars = ax4.barh(range(len(top_features)), top_features['importance'], color=colors)\n",
    "ax4.set_yticks(range(len(top_features)))\n",
    "ax4.set_yticklabels(top_features['feature'])\n",
    "ax4.set_xlabel('Importance Score', fontweight='bold')\n",
    "ax4.set_title('Top 15 Feature Importance', fontweight='bold', fontsize=14)\n",
    "ax4.invert_yaxis()\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 5: Prediction Probability Distribution\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "ax5.hist(y_pred_proba_best[y_test == 0], bins=30, alpha=0.6, \n",
    "         label='Human', color='#1f77b4', density=True)\n",
    "ax5.hist(y_pred_proba_best[y_test == 1], bins=30, alpha=0.6, \n",
    "         label='AI', color='#d62728', density=True)\n",
    "ax5.axvline(0.5, color='black', linestyle='--', linewidth=2, label='Threshold')\n",
    "ax5.set_xlabel('Predicted Probability (AI)', fontweight='bold')\n",
    "ax5.set_ylabel('Density', fontweight='bold')\n",
    "ax5.set_title('Prediction Confidence Distribution', fontweight='bold', fontsize=12)\n",
    "ax5.legend()\n",
    "ax5.grid(alpha=0.3)\n",
    "\n",
    "# Plot 6: Model Comparison (Bar Chart)\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "model_names = list(results.keys())\n",
    "roc_scores = [results[name]['roc_auc'] for name in model_names]\n",
    "f1_scores = [results[name]['f1'] for name in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "ax6.bar(x - width/2, roc_scores, width, label='ROC-AUC', color='skyblue')\n",
    "ax6.bar(x + width/2, f1_scores, width, label='F1-Score', color='coral')\n",
    "\n",
    "ax6.set_ylabel('Score', fontweight='bold')\n",
    "ax6.set_title('Model Performance Comparison', fontweight='bold', fontsize=12)\n",
    "ax6.set_xticks(x)\n",
    "ax6.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "ax6.legend()\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "ax6.set_ylim(0.7, 1.0)\n",
    "\n",
    "# Plot 7: Error Analysis\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "fp_indices = (y_test == 0) & (y_pred_best == 1)\n",
    "fn_indices = (y_test == 1) & (y_pred_best == 0)\n",
    "\n",
    "ax7.scatter(y_pred_proba_best[~(fp_indices | fn_indices)], \n",
    "           y_test[~(fp_indices | fn_indices)], \n",
    "           alpha=0.3, s=20, color='green', label='Correct')\n",
    "ax7.scatter(y_pred_proba_best[fp_indices], \n",
    "           y_test[fp_indices], \n",
    "           alpha=0.6, s=50, color='red', marker='x', label='False Positive')\n",
    "ax7.scatter(y_pred_proba_best[fn_indices], \n",
    "           y_test[fn_indices], \n",
    "           alpha=0.6, s=50, color='orange', marker='x', label='False Negative')\n",
    "ax7.axvline(0.5, color='black', linestyle='--', linewidth=1)\n",
    "ax7.set_xlabel('Predicted Probability (AI)', fontweight='bold')\n",
    "ax7.set_ylabel('True Label', fontweight='bold')\n",
    "ax7.set_yticks([0, 1])\n",
    "ax7.set_yticklabels(['Human', 'AI'])\n",
    "ax7.set_title('Error Analysis', fontweight='bold', fontsize=12)\n",
    "ax7.legend()\n",
    "ax7.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('AI Text Detector - Complete Performance Analysis', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "# --- 3. SALVATAGGIO IMMAGINE ---\n",
    "image_path = os.path.join('immagini', 'model_performance_analysis.png')\n",
    "plt.savefig(image_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Visualizzazione salvata: '{image_path}'\")\n",
    "plt.show()\n",
    "\n",
    "# --- 4. SALVATAGGIO MODELLI E ARTEFATTI ---\n",
    "print(\"\\n[9/9] Salvataggio modelli e artefatti...\")\n",
    "\n",
    "# Miglior modello (PKL)\n",
    "model_filename = f'best_model_{best_model_name.replace(\" \", \"_\")}.pkl'\n",
    "joblib.dump(best_model, os.path.join('pkl', model_filename))\n",
    "print(f\"‚úÖ Modello salvato: 'pkl/{model_filename}'\")\n",
    "\n",
    "# Scaler (PKL)\n",
    "joblib.dump(scaler, os.path.join('pkl', 'scaler.pkl'))\n",
    "print(\"‚úÖ Scaler salvato: 'pkl/scaler.pkl'\")\n",
    "\n",
    "# Feature List (TXT)\n",
    "with open(os.path.join('txt', 'feature_list.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(available_features))\n",
    "print(\"‚úÖ Feature list salvata: 'txt/feature_list.txt'\")\n",
    "\n",
    "# Feature Importance (CSV in cartella TXT)\n",
    "feature_importance_data.to_csv(os.path.join('txt', 'feature_importance.csv'), index=False)\n",
    "print(\"‚úÖ Feature importance salvata: 'txt/feature_importance.csv'\")\n",
    "\n",
    "print(\"\\nüöÄ Pipeline completata con successo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856b7d6",
   "metadata": {},
   "source": [
    "## Salvataggio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614245af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[9/9] Salvataggio modelli e artefatti...\n",
      "‚úÖ Modello salvato: 'best_model_Random_Forest.pkl'\n",
      "‚úÖ Scaler salvato: 'scaler.pkl'\n",
      "‚úÖ Feature list salvata: 'feature_list.txt'\n",
      "‚úÖ Feature importance salvata: 'feature_importance.csv'\n"
     ]
    }
   ],
   "source": [
    "\"\"\" print(\"\\n[9/9] Salvataggio modelli e artefatti...\")\n",
    "\n",
    "# Salva il miglior modello\n",
    "joblib.dump(best_model, f'best_model_{best_model_name.replace(\" \", \"_\")}.pkl')\n",
    "print(f\"‚úÖ Modello salvato: 'best_model_{best_model_name.replace(' ', '_')}.pkl'\")\n",
    "\n",
    "# Salva lo scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"‚úÖ Scaler salvato: 'scaler.pkl'\")\n",
    "\n",
    "# Salva feature list\n",
    "with open('feature_list.txt', 'w') as f:\n",
    "    f.write('\\n'.join(available_features))\n",
    "print(\"‚úÖ Feature list salvata: 'feature_list.txt'\") \"\"\"\n",
    "\n",
    "# Salva feature importance\n",
    "feature_importance_data.to_csv('feature_importance.csv', index=False)\n",
    "print(\"‚úÖ Feature importance salvata: 'feature_importance.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2091f946",
   "metadata": {},
   "source": [
    "## riepilogo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f4734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ PIPELINE COMPLETATA CON SUCCESSO!\n",
      "======================================================================\n",
      "\n",
      "üèÜ MIGLIOR MODELLO: Random Forest\n",
      "   ‚Ä¢ ROC-AUC:        1.0000\n",
      "   ‚Ä¢ F1-Score:       0.9899\n",
      "   ‚Ä¢ CV ROC-AUC:     0.9982 ¬± 0.0035\n",
      "   ‚Ä¢ Sensitivity:    0.9800\n",
      "   ‚Ä¢ Specificity:    1.0000\n",
      "\n",
      "üìÅ FILE GENERATI:\n",
      "   ‚Ä¢ model_performance_analysis.png\n",
      "   ‚Ä¢ best_model_Random_Forest.pkl\n",
      "   ‚Ä¢ scaler.pkl\n",
      "   ‚Ä¢ feature_list.txt\n",
      "   ‚Ä¢ feature_importance.csv\n",
      "\n",
      "üîç ANALISI DEGLI ERRORI:\n",
      "   ‚Ä¢ False Positives (Human ‚Üí AI): 0\n",
      "   ‚Ä¢ False Negatives (AI ‚Üí Human): 1\n",
      "   ‚Ä¢ Total Errors: 1 / 100 (1.00%)\n",
      "\n",
      "üéØ PROSSIMI STEP SUGGERITI:\n",
      "   1. Analisi SHAP values per interpretabilit√†\n",
      "   2. Test su dataset esterni (robustness check)\n",
      "   3. Analisi degli errori per identificare pattern problematici\n",
      "   4. Hyperparameter tuning con GridSearchCV/Optuna\n",
      "   5. Ensemble di modelli per migliorare performance\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PIPELINE COMPLETATA CON SUCCESSO!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüèÜ MIGLIOR MODELLO: {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ ROC-AUC:        {results[best_model_name]['roc_auc']:.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score:       {results[best_model_name]['f1']:.4f}\")\n",
    "print(f\"   ‚Ä¢ CV ROC-AUC:     {results[best_model_name]['cv_mean']:.4f} ¬± {results[best_model_name]['cv_std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Sensitivity:    {sensitivity:.4f}\")\n",
    "print(f\"   ‚Ä¢ Specificity:    {specificity:.4f}\")\n",
    "\n",
    "print(\"\\nüìÅ FILE GENERATI:\")\n",
    "print(\"   ‚Ä¢ model_performance_analysis.png\")\n",
    "print(f\"   ‚Ä¢ best_model_{best_model_name.replace(' ', '_')}.pkl\")\n",
    "print(\"   ‚Ä¢ scaler.pkl\")\n",
    "print(\"   ‚Ä¢ feature_list.txt\")\n",
    "print(\"   ‚Ä¢ feature_importance.csv\")\n",
    "\n",
    "print(\"\\nüîç ANALISI DEGLI ERRORI:\")\n",
    "print(f\"   ‚Ä¢ False Positives (Human ‚Üí AI): {fp}\")\n",
    "print(f\"   ‚Ä¢ False Negatives (AI ‚Üí Human): {fn}\")\n",
    "print(f\"   ‚Ä¢ Total Errors: {fp + fn} / {len(y_test)} ({(fp+fn)/len(y_test)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nüéØ PROSSIMI STEP SUGGERITI:\")\n",
    "print(\"   1. Analisi SHAP values per interpretabilit√†\")\n",
    "print(\"   2. Test su dataset esterni (robustness check)\")\n",
    "print(\"   3. Analisi degli errori per identificare pattern problematici\")\n",
    "print(\"   4. Hyperparameter tuning con GridSearchCV/Optuna\")\n",
    "print(\"   5. Ensemble di modelli per migliorare performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
