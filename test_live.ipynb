{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d52888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Caricamento in corso...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- hapax_density\n- template_bias_score\nFeature names seen at fit time, yet now missing:\n- avg_sentence_length\n- burstiness_index\n- clause_density\n- contraction_density\n- dependency_depth_mean\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# --- PROVA QUI ---\u001b[39;00m\n\u001b[32m     85\u001b[39m testo = \u001b[33m'\u001b[39m\u001b[33mciao a tutti\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[43mpredict_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtesto\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mpredict_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     63\u001b[39m features_df = extract_live_features(text)\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Scaling\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m features_scaled = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Predizione probabilit√†\u001b[39;00m\n\u001b[32m     69\u001b[39m prob_ai = model.predict_proba(features_scaled)[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1075\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1072\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1074\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1087\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2929\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2846\u001b[39m     _estimator,\n\u001b[32m   2847\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2853\u001b[39m     **check_params,\n\u001b[32m   2854\u001b[39m ):\n\u001b[32m   2855\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2856\u001b[39m \n\u001b[32m   2857\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2927\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2928\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2929\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2930\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2787\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2785\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- hapax_density\n- template_bias_score\nFeature names seen at fit time, yet now missing:\n- avg_sentence_length\n- burstiness_index\n- clause_density\n- contraction_density\n- dependency_depth_mean\n- ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from textstat import flesch_reading_ease\n",
    "\n",
    "# 1. CARICAMENTO ASSET E MODELLI\n",
    "print(\"üîÑ Caricamento in corso...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = joblib.load('best_model_Random_Forest.pkl')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "def extract_live_features(text):\n",
    "    \"\"\"\n",
    "    Estrae le feature esattamente come nel tuo signature.ipynb,\n",
    "    includendo le nuove firme 'Hapax' e 'Template'.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    words = [t.text.lower() for t in doc if not t.is_punct and not t.is_space]\n",
    "    sentences = list(doc.sents)\n",
    "    \n",
    "    # --- Feature Standard (dal tuo modeling.ipynb) ---\n",
    "    avg_word_len = np.mean([len(w) for w in words]) if words else 0\n",
    "    lexical_diversity = len(set(words)) / len(words) if words else 0\n",
    "    sent_lengths = [len([t for t in s if not t.is_punct]) for s in sentences]\n",
    "    sent_len_std = np.std(sent_lengths) if len(sent_lengths) > 1 else 0\n",
    "    \n",
    "    # --- Nuove Firme Avanzate ---\n",
    "    # Hapax Density\n",
    "    word_counts = Counter(words)\n",
    "    hapax_count = sum(1 for w in word_counts if word_counts[w] == 1)\n",
    "    hapax_density = hapax_count / len(words) if words else 0\n",
    "    \n",
    "    # Template Bias Score\n",
    "    t_score = 0\n",
    "    if any(re.search(p, text, re.MULTILINE) for p in [r'^\\s*[\\-\\*‚Ä¢]\\s+', r'^\\s*\\d+[\\.\\)]\\s+']):\n",
    "        t_score += 1.5\n",
    "    if any(p in text.lower() for p in ['in conclusion', 'overall', 'to summarize']):\n",
    "        t_score += 1.2\n",
    "        \n",
    "    # --- Altre feature presenti nel tuo dataset ---\n",
    "    flesch = flesch_reading_ease(text)\n",
    "    first_person = len(re.findall(r'\\b(i|my|me|mine|we|our)\\b', text.lower()))\n",
    "    \n",
    "    # CREAZIONE DATAFRAME (L'ordine deve essere identico a quello del training!)\n",
    "    # Nota: Assicurati che i nomi qui sotto siano identici a quelli in 'feature_list.txt'\n",
    "    feat_dict = {\n",
    "        'avg_word_length': avg_word_len,\n",
    "        'lexical_diversity': lexical_diversity,\n",
    "        'sentence_length_variability': sent_len_std,\n",
    "        'hapax_density': hapax_density,\n",
    "        'template_bias_score': t_score,\n",
    "        'readability_score': flesch,\n",
    "        'first_person_count': first_person\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([feat_dict])\n",
    "\n",
    "def predict_text(text):\n",
    "    # Estrazione\n",
    "    features_df = extract_live_features(text)\n",
    "    \n",
    "    # Scaling\n",
    "    features_scaled = scaler.transform(features_df)\n",
    "    \n",
    "    # Predizione probabilit√†\n",
    "    prob_ai = model.predict_proba(features_scaled)[0][1]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üî¨ DIAGNOSI STILOMETRICA\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Probabilit√† IA: {prob_ai*100:.2f}%\")\n",
    "    \n",
    "    if prob_ai > 0.5:\n",
    "        print(\"\\nRISULTATO: ü§ñ Testo generato da IA\")\n",
    "        print(\"Sospetti principali: Ritmo troppo regolare o vocabolario poco vario.\")\n",
    "    else:\n",
    "        print(\"\\nRISULTATO: ‚úçÔ∏è Testo scritto da un Umano\")\n",
    "        print(\"Segni distintivi: Imperfezioni naturali e alta ricchezza lessicale.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# --- PROVA QUI ---\n",
    "testo = 'ciao a tutti'\n",
    "predict_text(testo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
